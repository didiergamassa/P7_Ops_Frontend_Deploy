{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4635d7",
   "metadata": {},
   "source": [
    "# ***** NOTEBOOK 1 : Analyse Exploratoire des données principales*******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42f9ec",
   "metadata": {},
   "source": [
    "# 1 Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\n",
    "#pd.options.plotting.backend = \"plotly\"\n",
    "import numpy as np # fundamental package for scientific computing with Python\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for making plots with seaborn\n",
    "color = sns.color_palette()\n",
    "import plotly as py\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_row',250)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8214d",
   "metadata": {},
   "source": [
    "# 2 Importation jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.read_csv('application_train.csv')\n",
    "application_test = pd.read_csv('application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "application_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ca70e",
   "metadata": {},
   "source": [
    "# 3. Traitement des variables non numériques\n",
    "Les machines ne peuvent comprendre que les chiffres. Convertissons donc toutes les colonnes non numériques en nombres. Les variables catégorielles seront converties en colonnes fictives, les variables ordinales sont converties en nombres par mappage et les variables qui ne sont pas numériques et ne peuvent pas être converties en nombres seront supprimées du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of non-numerical variables\n",
    "application_train.select_dtypes(include=['O']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous ne pouvons pas avoir de colonnes non numériques pour la modélisation. Nous ne pouvons avoir que des colonnes numériques. Les colonnes non numériques peuvent également être des variables ordinales ou catégorielles.\n",
    "col_for_dummies=application_train.select_dtypes(include=['O']).columns.drop(['FLAG_OWN_CAR','FLAG_OWN_REALTY','EMERGENCYSTATE_MODE'])\n",
    "application_train_dummies = pd.get_dummies(application_train, columns = col_for_dummies, drop_first = True)\n",
    "application_test_dummies = pd.get_dummies(application_test, columns = col_for_dummies, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1073ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train_dummies.select_dtypes(include=['O']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09800a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train_dummies['EMERGENCYSTATE_MODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous ne pouvons pas convertir flag_own_car et flag_own_realty en colonne avec oui ou non, etc. Faisons plutôt correspondre oui à 1 et non à 0\n",
    "application_train_dummies['FLAG_OWN_CAR'] = application_train_dummies['FLAG_OWN_CAR'].map( {'Y':1, 'N':0})\n",
    "application_train_dummies['FLAG_OWN_REALTY'] = application_train_dummies['FLAG_OWN_REALTY'].map( {'Y':1, 'N':0})\n",
    "application_train_dummies['EMERGENCYSTATE_MODE'] = application_train_dummies['EMERGENCYSTATE_MODE'].map( {'Yes':1, 'No':0})\n",
    "\n",
    "application_test_dummies['FLAG_OWN_CAR'] = application_train_dummies['FLAG_OWN_CAR'].map( {'Y':1, 'N':0})\n",
    "application_test_dummies['FLAG_OWN_REALTY'] = application_train_dummies['FLAG_OWN_REALTY'].map( {'Y':1, 'N':0})\n",
    "application_test_dummies['EMERGENCYSTATE_MODE'] = application_train_dummies['EMERGENCYSTATE_MODE'].map( {'Yes':1, 'No':0})\n",
    "print(application_train_dummies.shape)\n",
    "print(application_test_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous avons 4 colonnes de moins dans application_test_dummies. Voyons quelles sont ces 4 colonnes\n",
    "#Parfois, les données de test n'ont pas certaines colonnes.\n",
    "application_train_dummies.columns.difference(application_test_dummies.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d506307",
   "metadata": {},
   "source": [
    "# 4. Alignement des jeux de donnés d'entrainement et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d571ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Il doit y avoir les mêmes caractéristiques (colonnes) dans les données d'entrainement et de test. L'encodage à chaud a créé plus de colonnes dans les données d'apprentissage car il y avait des variables catégorielles avec des catégories non représentées dans les données de test. Pour supprimer les colonnes des données d'entraînement qui ne figurent pas dans les données de test, nous devons aligner les dataframes. Nous extrayons d'abord la colonne cible des données d'apprentissage (car cela ne figure pas dans les données de test, mais nous devons conserver ces informations). Lorsque nous faisons l'alignement, nous devons nous assurer de définir axis = 1 pour aligner les dataframes en fonction des colonnes et non des lignes !\n",
    "train_labels = application_train_dummies['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "application_train_dummies, application_test_dummies = application_train_dummies.align(application_test_dummies, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target back in\n",
    "application_train_dummies['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape: ', application_train_dummies.shape)\n",
    "print('Testing Features shape: ', application_test_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209b57a",
   "metadata": {},
   "source": [
    "###### The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try dimensionality reduction (removing features that are not relevant) to reduce the size of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b932f5",
   "metadata": {},
   "source": [
    "# 5 Traitement des valeurs manquantes (à l'aide d'un imputer itératif) avant  détection des valeurs aberrantes\n",
    "Nous devons gérer nos valeurs manquantes avant de pouvoir effectuer tout type de détection de valeurs aberrantes. Il existe de nombreuses façons de gérer les valeurs manquantes. Nous pouvons utiliser fillna() et remplacer les valeurs manquantes par la moyenne, la médiane ou la valeur la plus fréquente des données. L'approche que nous utiliserons ci-dessous sera Iterative Imputer. L'imputateur itératif considérera la variable manquante comme la variable dépendante et toutes les autres caractéristiques seront des variables indépendantes. Il y aura donc une régression et les variables indépendantes seront utilisées pour déterminer la variable dépendante (qui est la caractéristique manquante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c362923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d042ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=application_train_dummies[['SK_ID_CURR','TARGET']]\n",
    "X=application_train_dummies.drop(columns=['TARGET'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcba87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputation = X.loc[:, (X.nunique() > 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a96c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c07a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(BayesianRidge())\n",
    "imputed_total = pd.DataFrame(imputer.fit_transform(X_imputation))\n",
    "imputed_total.columns = X_imputation.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11320d08",
   "metadata": {},
   "source": [
    "# 6 Détection des valeurs aberrantes\n",
    "En statistique, une valeur aberrante est un point d'observation éloigné des autres observations. Il existe de nombreuses façons de détecter les valeurs aberrantes.\n",
    "\n",
    "Méthodes visuelles pour repérer et supprimer les valeurs aberrantes\n",
    "\n",
    "Boîte à moustaches\n",
    "Nuages ​​de points\n",
    "Détection et suppression des valeurs aberrantes à l'aide d'une fonction mathématique\n",
    "\n",
    "Z-score : un seuil de -3 à 3 est pris, et tout point avec un score z non compris dans cette plage est supprimé en tant que valeur aberrante.\n",
    "Score IQR : Cela fonctionne comme un diagramme en boîte et un score z en ce sens qu'une valeur seuil IQR est définie. L'IQR est le premier quartile soustrait du troisième quartile. Tout point en dessous du seuil IQR est supprimé.\n",
    "\n",
    "##### Méthodes de classification pour la détection des valeurs aberrantes\n",
    "\n",
    "Clustering DBScan (création de clusters autour de points de données). Un nombre minimum de points est requis pour être dans un cluster. Il y aura des points qui n'appartiennent à aucun cluster ou bien des points qui sont uniques dans un cluster entier. Nous pouvons donc supprimer ces points de bruit.\n",
    "Forêt d'isolement : la forêt d'isolement produira les prédictions pour chaque point de données dans un tableau. Si le résultat est -1, cela signifie que ce point de données spécifique est une valeur aberrante. Si le résultat est 1, cela signifie que le point de données n'est pas une valeur aberrante\n",
    "Ici, nous utiliserons la méthode Isolation Forest car elle peut bien gérer les valeurs manquantes et ne nécessite pas de mise à l'échelle des entrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "rs=np.random.RandomState(0)\n",
    "clf = IsolationForest(max_samples=100,random_state=rs, contamination=.1) \n",
    "clf.fit(imputed_total)\n",
    "if_scores = clf.decision_function(imputed_total)\n",
    "\n",
    "pred = clf.predict(imputed_total)\n",
    "imputed_total['anomaly']=pred\n",
    "outliers=imputed_total.loc[imputed_total['anomaly']==-1]\n",
    "outlier_index=list(outliers.index)\n",
    "#print(outlier_index)\n",
    "#Find the number of anomalies and normal points here points classified -1 are anomalous\n",
    "print(imputed_total['anomaly'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_ID=list(outliers['SK_ID_CURR'])\n",
    "X_new = X[~X.SK_ID_CURR.isin(outlier_ID)]\n",
    "y_new = y[~y.SK_ID_CURR.isin(outlier_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_new.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2f129",
   "metadata": {},
   "source": [
    "### 6.1 Détection des anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961cf58",
   "metadata": {},
   "source": [
    "Bien que nous ayons supprimé les valeurs aberrantes à l'aide de la forêt d'isolement, nous verrons toujours les données une fois pour vérifier toute anomalie. La forêt d'isolement ou toute méthode de détection de valeurs aberrantes suppose que la valeur aberrante est un point minoritaire et ne ressemble pas aux autres points majoritaires. Cependant, certains points d'aberration sont parfois trop nombreux. Voyons s'il existe une telle anamolie que nous trouvons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24a66c",
   "metadata": {},
   "source": [
    "# 7 Détection univariée des valeurs aberrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afccd80d",
   "metadata": {},
   "source": [
    "##### Negative numbers: DAYS_BIRTH, DAYS_EMPLOYED, DAYS_REGISTRATION, DAYS_ID_PUBLISH, DAYS_LAST_PHONE_CHANGE\n",
    "Univariate outliers detection\n",
    "\n",
    "Negative numbers: DAYS_BIRTH, DAYS_EMPLOYED, DAYS_REGISTRATION, DAYS_ID_PUBLISH, DAYS_LAST_PHONE_CHANGE\n",
    "\n",
    "Numbers are negative since they are taken relative to the date of application. So we need to change them to positive.\n",
    "\n",
    "#### Maximum value discrepancy Days_EMPLOYED:365243 days(over 1000years) OWN_CAR_AGE:91 yEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the anamalous variables values in years\n",
    "print('DAYS_BIRTH stats in years:','\\n',(X_new['DAYS_BIRTH'] / -365).describe(),'\\n')\n",
    "print('Check the stats in years to see if there is any anomalous behavior')\n",
    "print('DAYS_EMPLOYED stats in years:','\\n',(X_new['DAYS_EMPLOYED'] / -365).describe(),'\\n')\n",
    "print('DAYS_REGISTRATION stats in years:','\\n',(X_new['DAYS_REGISTRATION'] / -365).describe(),'\\n')\n",
    "print('DAYS_ID_PUBLISH stats in years:','\\n',(X_new['DAYS_ID_PUBLISH'] / -365).describe(),'\\n')\n",
    "print('DAYS_LAST_PHONE_CHANGE stats in years:','\\n',(X_new['DAYS_LAST_PHONE_CHANGE'] / -365).describe(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff17cc",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, il y a une anomalie dans Days_employed car il est très peu probable qu'une personne soit employée pendant 1000 ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new['DAYS_EMPLOYED'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c788eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the error values in Days_employed with nan\n",
    "X_new['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "application_test_dummies['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab508dc5",
   "metadata": {},
   "source": [
    "Les données sont maintenant belles et propres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb729bd",
   "metadata": {},
   "source": [
    "# 8.Données manquantes dans la table de donné application_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c18de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing data\n",
    "total = X_new.isnull().sum().sort_values(ascending = False)\n",
    "percent = (X_new.isnull().sum()/X_new.isnull().count()*100).sort_values(ascending = False)\n",
    "missing_application_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_application_train_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46457b8a",
   "metadata": {},
   "source": [
    "# 9.Duplicate data in application_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_without_id = [col for col in X_new.columns if col!='SK_ID_CURR']\n",
    "#Checking for duplicates in the data.\n",
    "X_new[X_new.duplicated(subset = columns_without_id, keep=False)]\n",
    "print('The no of duplicates in the data:',X_new[X_new.duplicated(subset = columns_without_id, keep=False)]\n",
    "      .shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d92a5b",
   "metadata": {},
   "source": [
    "# 10 Vérification du déséquilibre des données cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3419d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6173ad5",
   "metadata": {},
   "source": [
    "On voit que la classe est clairement déséquilibrée avec des cas de défaut très faibles par rapport à l'ensemble des cas. Nous devons donc équilibrer les données lorsque nous utilisons des modèles d'apprentissage automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65704c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Des=pd.DataFrame(y_new['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Loan Secured', 'Loan Unsecured'\n",
    "sizes = Des.TARGET\n",
    "colors = ['lightskyblue','lightcoral']\n",
    "fig = plt.figure(figsize =(3, 3))\n",
    "plt.pie(sizes, labels=labels, colors=colors, \n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.savefig('PieChart01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.to_csv('X_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f970b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new.to_csv('y_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64c2a4",
   "metadata": {},
   "source": [
    "# 11.Analyse exploratoire par visualisation de la distribution des variables de la table de donnée principale: application_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894962fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a9431",
   "metadata": {},
   "source": [
    "## 11.1 Distribution des revenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69beebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # for making plots with seaborn\n",
    "color = sns.color_palette()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Distribution of AMT_INCOME_TOTAL\")\n",
    "ax = sns.distplot(X_new[\"AMT_INCOME_TOTAL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91889b3",
   "metadata": {},
   "source": [
    "La distribution est asymétrique à droite et il y a des valeurs extrêmes, nous pouvons appliquer la distribution logarithmique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c39ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[\"AMT_INCOME_TOTAL\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train=pd.merge(X_new,y_new,on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(application_train[application_train['AMT_INCOME_TOTAL'] > 1000000]['TARGET'].value_counts())/len(application_train[application_train['AMT_INCOME_TOTAL'] > 1000000])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5a2a2",
   "metadata": {},
   "source": [
    "Les personnes à revenu élevé ont tendance à ne pas etre en  défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxcox=0 means we are taking log transformation of data to show it as normal form\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "np.log(application_train['AMT_INCOME_TOTAL']).iplot(kind='histogram', bins=100,\n",
    "                               xTitle = 'log(INCOME_TOTAL)',yTitle ='Count corresponding to Incomes',\n",
    "                               title='Distribution of log(AMT_INCOME_TOTAL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32778c1b",
   "metadata": {},
   "source": [
    "Nous voyons que la variable de revenu obtient une distribution normale lorsqu'elle est transformée en log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f898c9",
   "metadata": {},
   "source": [
    "## 11.2. Répartition du crédit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # for making plots with seaborn\n",
    "color = sns.color_palette()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Distribution of AMT_CREDIT\")\n",
    "ax = sns.distplot(application_train[\"AMT_CREDIT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b765632",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train[\"AMT_CREDIT\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce10b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "(application_train[application_train['AMT_CREDIT']>2000000]['TARGET'].value_counts())/len(application_train[application_train['AMT_CREDIT']>2000000])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a3de6",
   "metadata": {},
   "source": [
    "Les gens qui prennent moins de défaut de crédit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34284319",
   "metadata": {},
   "source": [
    "## 11.3. Répartition des types de prêts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f21a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_data = pd.read_csv('application_train.csv')\n",
    "\n",
    "contract_val = original_train_data['NAME_CONTRACT_TYPE'].value_counts()\n",
    "contract_df = pd.DataFrame({'labels': contract_val.index,\n",
    "                   'values': contract_val.values\n",
    "                  })\n",
    "contract_df.iplot(kind='pie',labels='labels',values='values', title='Types of Loan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f0006",
   "metadata": {},
   "source": [
    "Plus de gens sont intéressés à contracter des prêts en espèces que des prêts renouvelables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762a551",
   "metadata": {},
   "source": [
    "## 11.4 Distribution de type de revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b888f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_data[\"NAME_INCOME_TYPE\"].iplot(kind=\"histogram\", bins=20, theme=\"white\", title=\"Passenger's Income Types\",\n",
    "                                            xTitle='Name of Income Types', yTitle='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_val = original_train_data['NAME_INCOME_TYPE'].value_counts()\n",
    "\n",
    "education_val_y0 = []\n",
    "education_val_y1 = []\n",
    "for val in education_val.index:\n",
    "    education_val_y1.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_INCOME_TYPE']==val] == 1))\n",
    "    education_val_y0.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_INCOME_TYPE']==val] == 0))\n",
    "\n",
    "data = [go.Bar(x = education_val.index, y = ((education_val_y1 / education_val.sum()) * 100), name='Default' ),\n",
    "        go.Bar(x = education_val.index, y = ((education_val_y0 / education_val.sum()) * 100), name='No default' )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"Income of people affecting default on loans\",\n",
    "    xaxis=dict(\n",
    "        title='Income of people',\n",
    "       ),\n",
    "    yaxis=dict(\n",
    "        title='Count of people accompanying in %',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout=layout) \n",
    "fig.layout.template = 'plotly_dark'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f94d3",
   "metadata": {},
   "source": [
    "## 11.5 Distribution du Type d'accompagnateur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7d676",
   "metadata": {},
   "source": [
    "Qui a accompagné la personne lors de la souscription du prêt ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_data[\"NAME_TYPE_SUITE\"].iplot(kind=\"histogram\", bins=20, theme=\"white\", title=\"Accompanying Person\",\n",
    "                                            xTitle='People accompanying', yTitle='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b85fb",
   "metadata": {},
   "source": [
    "Most people are unaccompanied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549a7b0",
   "metadata": {},
   "source": [
    "## 11.6. Distribution du nom du type d'éducation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6970b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_val = original_train_data['NAME_EDUCATION_TYPE'].value_counts()\n",
    "\n",
    "education_val_y0 = []\n",
    "education_val_y1 = []\n",
    "for val in education_val.index:\n",
    "    education_val_y1.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_EDUCATION_TYPE']==val] == 1))\n",
    "    education_val_y0.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_EDUCATION_TYPE']==val] == 0))\n",
    "\n",
    "data = [go.Bar(x = education_val.index, y = ((education_val_y1 / education_val.sum()) * 100), name='Default' ),\n",
    "        go.Bar(x = education_val.index, y = ((education_val_y0 / education_val.sum()) * 100), name='No default' )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"Education sources of Applicants in terms of loan is repayed or not  in %\",\n",
    "    xaxis=dict(\n",
    "        title='Education of Applicants',\n",
    "       ),\n",
    "    yaxis=dict(\n",
    "        title='Count of applicants in %',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout=layout) \n",
    "fig.layout.template = 'plotly_dark'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d5d88",
   "metadata": {},
   "source": [
    "Les titulaires d'un diplôme sont en mesure de rembourser la plupart du temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589895b",
   "metadata": {},
   "source": [
    "## 11.7 Effet de l'état matrimonial sur la capacité de rembourser les prêts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3505e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_val = original_train_data['NAME_FAMILY_STATUS'].value_counts()\n",
    "\n",
    "education_val_y0 = []\n",
    "education_val_y1 = []\n",
    "for val in education_val.index:\n",
    "    education_val_y1.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_FAMILY_STATUS']==val] == 1))\n",
    "    education_val_y0.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_FAMILY_STATUS']==val] == 0))\n",
    "\n",
    "data = [go.Bar(x = education_val.index, y = ((education_val_y1 / education_val.sum()) * 100), name='Default' ),\n",
    "        go.Bar(x = education_val.index, y = ((education_val_y0 / education_val.sum()) * 100), name='No default' )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"Family status of Applicant in terms of loan is repayed or not in %\",\n",
    "    xaxis=dict(\n",
    "        title='Family status of Applicants',\n",
    "       ),\n",
    "    yaxis=dict(\n",
    "        title='Count of applicants in %',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout=layout) \n",
    "fig.layout.template = 'plotly_dark'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b63d4",
   "metadata": {},
   "source": [
    "## 11.8. Distribution du Type de Résidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_val = original_train_data['NAME_HOUSING_TYPE'].value_counts()\n",
    "\n",
    "education_val_y0 = []\n",
    "education_val_y1 = []\n",
    "for val in education_val.index:\n",
    "    education_val_y1.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_HOUSING_TYPE']==val] == 1))\n",
    "    education_val_y0.append(np.sum(original_train_data['TARGET'][original_train_data['NAME_HOUSING_TYPE']==val] == 0))\n",
    "\n",
    "data = [go.Bar(x = education_val.index, y = ((education_val_y1 / education_val.sum()) * 100), name='Default' ),\n",
    "        go.Bar(x = education_val.index, y = ((education_val_y0 / education_val.sum()) * 100), name='No default' )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"Housing Type of Applicant in terms of loan is repayed or not in %\",\n",
    "    xaxis=dict(\n",
    "        title='Housing Type of Applicants',\n",
    "       ),\n",
    "    yaxis=dict(\n",
    "        title='Count of applicants in %',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout=layout) \n",
    "fig.layout.template = 'plotly_dark'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395452f9",
   "metadata": {},
   "source": [
    "Les gens dans un appartement de bureau, un appartement coopératif ne font presque jamais défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181f159",
   "metadata": {},
   "source": [
    "## 11.9 Distribution de l'age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880699e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(original_train_data[\"DAYS_BIRTH\"]/-365).iplot(kind=\"histogram\", bins=20, theme=\"white\", title=\"Customer's Ages\",\n",
    "                                            xTitle='Age of customer', yTitle='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fefc71",
   "metadata": {},
   "source": [
    "## 11.10 Analyse visuelle du défaut en fonction de  la variable  OCCUPATION_TYPE(Statut Professionnel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_val = original_train_data['OCCUPATION_TYPE'].value_counts()\n",
    "\n",
    "parameter_val_y0 = []\n",
    "parameter_val_y1 = []\n",
    "for val in parameter_val.index:\n",
    "    parameter_val_y1.append(np.sum(original_train_data['TARGET'][original_train_data['OCCUPATION_TYPE']==val] == 1))\n",
    "    parameter_val_y0.append(np.sum(original_train_data['TARGET'][original_train_data['OCCUPATION_TYPE']==val] == 0))\n",
    "\n",
    "data = [go.Bar(x = parameter_val.index, y = ((parameter_val_y1 / parameter_val.sum()) * 100), name='Default' ),\n",
    "        go.Bar(x = parameter_val.index, y = ((parameter_val_y0 / parameter_val.sum()) * 100), name='No default' )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"Occupation type of people affecting default on loans\",\n",
    "    xaxis=dict(\n",
    "        title='Occupation type of people',\n",
    "       ),\n",
    "    yaxis=dict(\n",
    "        title='Count of people Occupation that type of housing in %',\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout=layout) \n",
    "fig.layout.template = 'plotly_dark'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497a3cc",
   "metadata": {},
   "source": [
    "Les personnes hautement qualifiées sont plus susceptibles de rembourser et les personnes peu qualifiées moins susceptibles de rembourser leurs prêts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf14cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "application_train_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fb7a1",
   "metadata": {},
   "source": [
    " # 12 Processus de combinaison des tables de données et extraction des données supplémentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51942cb5",
   "metadata": {},
   "source": [
    "## 12.1 Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5b9f0",
   "metadata": {},
   "source": [
    "### 12.1.1 Feature Engineering de la table de données Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flag to represent when credit > income\n",
    "#Train\n",
    "application_train_dummies['Credit_flag'] = application_train_dummies['AMT_INCOME_TOTAL'] > application_train_dummies['AMT_CREDIT']\n",
    "application_train_dummies['Percent_Days_employed'] = application_train_dummies['DAYS_EMPLOYED']/application_train_dummies['DAYS_BIRTH']*100\n",
    "application_train_dummies['Annuity_as_percent_income'] = application_train_dummies['AMT_ANNUITY']/ application_train_dummies['AMT_INCOME_TOTAL']*100\n",
    "application_train_dummies['Credit_as_percent_income'] = application_train_dummies['AMT_CREDIT']/application_train_dummies['AMT_INCOME_TOTAL']*100\n",
    "#Test\n",
    "application_test_dummies['Credit_flag'] = application_test_dummies['AMT_INCOME_TOTAL'] > application_test_dummies['AMT_CREDIT']\n",
    "application_test_dummies['Percent_Days_employed'] = application_test_dummies['DAYS_EMPLOYED']/application_test_dummies['DAYS_BIRTH']*100\n",
    "application_test_dummies['Annuity_as_percent_income'] = application_test_dummies['AMT_ANNUITY']/ application_test_dummies['AMT_INCOME_TOTAL']*100\n",
    "application_test_dummies['Credit_as_percent_income'] = application_test_dummies['AMT_CREDIT']/application_test_dummies['AMT_INCOME_TOTAL']*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f323434",
   "metadata": {},
   "source": [
    "### 12.1.2 Feature engineering de la table de données Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0495b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance = pd.read_csv('credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining numerical features\n",
    "bureau = pd.read_csv('bureau.csv')\n",
    "grp = bureau.drop(['SK_ID_BUREAU'], axis = 1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "grp.columns = ['BUREAU_'+column if column !='SK_ID_CURR' else column for column in grp.columns]\n",
    "application_bureau = application_train_dummies.merge(grp, on='SK_ID_CURR', how='left')\n",
    "application_bureau.update(application_bureau[grp.columns].fillna(0))\n",
    "\n",
    "application_bureau_test = application_test_dummies.merge(grp, on='SK_ID_CURR', how='left')\n",
    "application_bureau_test.update(application_bureau_test[grp.columns].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining categorical features\n",
    "bureau_categorical = pd.get_dummies(bureau.select_dtypes('object'))\n",
    "bureau_categorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\n",
    "grp = bureau_categorical.groupby(by = ['SK_ID_CURR']).mean().reset_index()\n",
    "grp.columns = ['BUREAU_'+column if column !='SK_ID_CURR' else column for column in grp.columns]\n",
    "application_bureau = application_bureau.merge(grp, on='SK_ID_CURR', how='left')\n",
    "application_bureau.update(application_bureau[grp.columns].fillna(0))\n",
    "\n",
    "application_bureau_test = application_bureau_test.merge(grp, on='SK_ID_CURR', how='left')\n",
    "application_bureau_test.update(application_bureau_test[grp.columns].fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558547f9",
   "metadata": {},
   "source": [
    "### 12.1.3 Feature Engineering de la table de données Bureau Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0699f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of types of past loans per customer \n",
    "grp = bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by = ['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(columns={'CREDIT_TYPE': 'BUREAU_LOAN_TYPES'})\n",
    "\n",
    "application_bureau = application_bureau.merge(grp, on='SK_ID_CURR', how='left')\n",
    "application_bureau['BUREAU_LOAN_TYPES'] = application_bureau['BUREAU_LOAN_TYPES'].fillna(0)\n",
    "\n",
    "application_bureau_test = application_bureau_test.merge(grp, on='SK_ID_CURR', how='left')\n",
    "application_bureau_test['BUREAU_LOAN_TYPES'] = application_bureau_test['BUREAU_LOAN_TYPES'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debt over credit ratio \n",
    "bureau['AMT_CREDIT_SUM'] = bureau['AMT_CREDIT_SUM'].fillna(0)\n",
    "bureau['AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)\n",
    "\n",
    "grp1 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM': 'TOTAL_CREDIT_SUM'})\n",
    "\n",
    "grp2 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT':'TOTAL_CREDIT_SUM_DEBT'})\n",
    "\n",
    "grp1['DEBT_CREDIT_RATIO'] = grp2['TOTAL_CREDIT_SUM_DEBT']/grp1['TOTAL_CREDIT_SUM']\n",
    "\n",
    "del grp1['TOTAL_CREDIT_SUM']\n",
    "\n",
    "application_bureau = application_bureau.merge(grp1, on='SK_ID_CURR', how='left')\n",
    "application_bureau['DEBT_CREDIT_RATIO'] = application_bureau['DEBT_CREDIT_RATIO'].fillna(0)\n",
    "application_bureau['DEBT_CREDIT_RATIO'] = application_bureau['DEBT_CREDIT_RATIO'].replace([np.inf, -np.inf], 0)\n",
    "application_bureau['DEBT_CREDIT_RATIO'] = pd.to_numeric(application_bureau['DEBT_CREDIT_RATIO'], downcast='float')\n",
    "\n",
    "application_bureau_test = application_bureau_test.merge(grp1, on='SK_ID_CURR', how='left')\n",
    "application_bureau_test['DEBT_CREDIT_RATIO'] = application_bureau_test['DEBT_CREDIT_RATIO'].fillna(0)\n",
    "application_bureau_test['DEBT_CREDIT_RATIO'] = application_bureau_test['DEBT_CREDIT_RATIO'].replace([np.inf, -np.inf], 0)\n",
    "application_bureau_test['DEBT_CREDIT_RATIO'] = pd.to_numeric(application_bureau_test['DEBT_CREDIT_RATIO'], downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overdue over debt ratio\n",
    "bureau['AMT_CREDIT_SUM_OVERDUE'] = bureau['AMT_CREDIT_SUM_OVERDUE'].fillna(0)\n",
    "bureau['AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)\n",
    "\n",
    "grp1 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM_OVERDUE']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_OVERDUE'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_OVERDUE': 'TOTAL_CUSTOMER_OVERDUE'})\n",
    "\n",
    "grp2 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT':'TOTAL_CUSTOMER_DEBT'})\n",
    "\n",
    "grp1['OVERDUE_DEBT_RATIO'] = grp1['TOTAL_CUSTOMER_OVERDUE']/grp2['TOTAL_CUSTOMER_DEBT']\n",
    "\n",
    "del grp1['TOTAL_CUSTOMER_OVERDUE']\n",
    "\n",
    "application_bureau = application_bureau.merge(grp1, on='SK_ID_CURR', how='left')\n",
    "application_bureau['OVERDUE_DEBT_RATIO'] = application_bureau['OVERDUE_DEBT_RATIO'].fillna(0)\n",
    "application_bureau['OVERDUE_DEBT_RATIO'] = application_bureau['OVERDUE_DEBT_RATIO'].replace([np.inf, -np.inf], 0)\n",
    "application_bureau['OVERDUE_DEBT_RATIO'] = pd.to_numeric(application_bureau['OVERDUE_DEBT_RATIO'], downcast='float')\n",
    "\n",
    "application_bureau_test = application_bureau_test.merge(grp1, on='SK_ID_CURR', how='left')\n",
    "application_bureau_test['OVERDUE_DEBT_RATIO'] = application_bureau_test['OVERDUE_DEBT_RATIO'].fillna(0)\n",
    "application_bureau_test['OVERDUE_DEBT_RATIO'] = application_bureau_test['OVERDUE_DEBT_RATIO'].replace([np.inf, -np.inf], 0)\n",
    "application_bureau_test['OVERDUE_DEBT_RATIO'] = pd.to_numeric(application_bureau_test['OVERDUE_DEBT_RATIO'], downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dadd0bf",
   "metadata": {},
   "source": [
    "### 12.1.4 Feature engineering de la table de données Previous Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260729e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOneToOne(df, col1, col2):\n",
    "    first = df.drop_duplicates([col1, col2]).groupby(col1)[col2].count().max()\n",
    "    second = df.drop_duplicates([col1, col2]).groupby(col2)[col1].count().max()\n",
    "    return first + second == 2\n",
    "previous_application = pd.read_csv('previous_application.csv')\n",
    "isOneToOne(previous_application,'SK_ID_CURR','SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dee231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of previous applications per customer\n",
    "grp = previous_application[['SK_ID_CURR','SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV':'PREV_APP_COUNT'})\n",
    "\n",
    "# Take only the IDs which are present in application_bureau\n",
    "application_bureau_prev = application_bureau.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "application_bureau_prev_test = application_bureau_test.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "#Fill NA for previous application counts (lets say there was an application ID present in application_bureau but not present\n",
    "# in grp, then that means that person never took loan previously, so count of previous loan for that person = 0)\n",
    "application_bureau_prev['PREV_APP_COUNT'] = application_bureau_prev['PREV_APP_COUNT'].fillna(0)\n",
    "application_bureau_prev_test['PREV_APP_COUNT'] = application_bureau_prev_test['PREV_APP_COUNT'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining numerical features\n",
    "\n",
    "#Take the mean of all the parameters (grouping by SK_ID_CURR)\n",
    "grp = previous_application.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "\n",
    "#Add prefix prev in front of all columns so that we know that these columns are from previous_application\n",
    "prev_columns = ['PREV_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\n",
    "\n",
    "#Change the columns\n",
    "grp.columns = prev_columns\n",
    "\n",
    "application_bureau_prev = application_bureau_prev.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "application_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))\n",
    "application_bureau_prev_test = application_bureau_prev_test.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "application_bureau_prev_test.update(application_bureau_prev_test[grp.columns].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b32e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbb296",
   "metadata": {},
   "source": [
    "### 12.1.5 Feature engineering  de la table de données installments_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_payments = pd.read_csv('installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining numerical features and there are no categorical features in this dataset\n",
    "grp = insta_payments.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "prev_columns = ['INSTA_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\n",
    "grp.columns = prev_columns\n",
    "application_bureau_prev = application_bureau_prev.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "application_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))\n",
    "application_bureau_prev_test = application_bureau_prev_test.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "application_bureau_prev_test.update(application_bureau_prev_test[grp.columns].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d689b9",
   "metadata": {},
   "source": [
    "### 12.1.6 Feature engineering  de la table de données Credit card balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39948660",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance = pd.read_csv('credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card=credit_card_balance\n",
    "# Combining numerical features\n",
    "grp = credit_card.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "prev_columns = ['CREDIT_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\n",
    "grp.columns = prev_columns\n",
    "application_bureau_prev = application_bureau_prev.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "application_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))\n",
    "\n",
    "application_bureau_prev_test = application_bureau_prev_test.merge(grp, on =['SK_ID_CURR'], how = 'left')\n",
    "application_bureau_prev_test.update(application_bureau_prev_test[grp.columns].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining categorical features\n",
    "credit_categorical = pd.get_dummies(credit_card.select_dtypes('object'))\n",
    "credit_categorical['SK_ID_CURR'] = credit_card['SK_ID_CURR']\n",
    "\n",
    "grp = credit_categorical.groupby('SK_ID_CURR').mean().reset_index()\n",
    "grp.columns = ['CREDIT_'+column if column != 'SK_ID_CURR' else column for column in grp.columns]\n",
    "\n",
    "application_bureau_prev = application_bureau_prev.merge(grp, on=['SK_ID_CURR'], how='left')\n",
    "application_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))\n",
    "\n",
    "application_bureau_prev_test = application_bureau_prev_test.merge(grp, on=['SK_ID_CURR'], how='left')\n",
    "application_bureau_prev_test.update(application_bureau_prev_test[grp.columns].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a07c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_bureau_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be9d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "application_bureau_prev_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5fd6d",
   "metadata": {},
   "source": [
    "# 13. Application Train_agg/Test_agg--->Data Final  Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa5393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "appli_train_agg=application_bureau_prev.copy()\n",
    "appli_test_agg=application_bureau_prev_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Sign=appli_train_agg['SK_ID_CURR']\n",
    "test_Sign=appli_test_agg['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6755fd",
   "metadata": {},
   "source": [
    "## 13.1 Séparation de la variable Target du training dataset aggrégé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agg = appli_train_agg['TARGET']\n",
    "X = appli_train_agg.drop('TARGET', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c9ac9",
   "metadata": {},
   "source": [
    "## 13.1Recombinaison des tables de données Train  et Test agregrées pour un preprocessing combiné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74937aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine application train and test tables for preprocessing.\n",
    "Data = pd.concat([X, appli_test_agg], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77074621",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b63cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940043c2",
   "metadata": {},
   "source": [
    "### 13.2 Quantification des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value\n",
    "def missing_values(data, plot=False):\n",
    "    mst = pd.DataFrame(\n",
    "        {\"Num_Missing\": data.isnull().sum(), \"Missing_Ratio\": data.isnull().sum() / data.shape[0]}).sort_values(\n",
    "        \"Num_Missing\", ascending=False)\n",
    "    mst[\"DataTypes\"] = data[mst.index].dtypes.values\n",
    "    mst = mst[mst.Num_Missing > 0].reset_index().rename({\"index\": \"Feature\"}, axis=1)\n",
    "\n",
    "    print(\"Number of Variables include Missing Values:\", mst.shape[0], \"\\n\")\n",
    "\n",
    "    if mst[mst.Missing_Ratio >= 1.0].shape[0] > 0:\n",
    "        print(\"Full Missing Variables:\", mst[mst.Missing_Ratio >= 1.0].Feature.tolist())\n",
    "        data.drop(mst[mst.Missing_Ratio >= 1.0].Feature.tolist(), axis=1, inplace=True)\n",
    "\n",
    "        print(\"Full missing variables are deleted!\", \"\\n\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(25, 8))\n",
    "        p = sns.barplot(mst.Missing_Ratio)\n",
    "        for rotate in p.get_xticklabels():\n",
    "            rotate.set_rotation(90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c28061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantification des valeurs manquantes\n",
    "missing_values(Data, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf85b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42588ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ecb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f58f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_factor(df, tx_threshold=30):\n",
    "  null_rate = ((df.isnull().sum() / df.shape[0])*100).sort_values(ascending=False).reset_index()\n",
    "  null_rate.columns = ['Variable','Taux_de_Null']\n",
    "  high_null_rate = null_rate[null_rate.Taux_de_Null >= tx_threshold]\n",
    "  return high_null_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea079dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_null_rate = null_factor(Data,30)\n",
    "full_null_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345eb25",
   "metadata": {},
   "source": [
    "# 14 Selection des features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129512a",
   "metadata": {},
   "source": [
    "## 14.1 Selection des colonnes avec des valeurs  nan inferieures ou ègal à 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb333d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "filling_features = null_factor(Data, 0)\n",
    "filling_features[\"Taux_de_Null\"] = 100-filling_features[\"Taux_de_Null\"]\n",
    "filling_features = filling_features.sort_values(\"Taux_de_Null\", ascending=False) \n",
    "\n",
    "#Seuil de suppression\n",
    "sup_threshold =70\n",
    "\n",
    "fig = plt.figure(figsize=(20, 35))\n",
    "\n",
    "font_title = {'family': 'serif',\n",
    "              'color':  '#114b98',\n",
    "              'weight': 'bold',\n",
    "              'size': 18,\n",
    "             }\n",
    "\n",
    "sns.barplot(x=\"Taux_de_Null\", y=\"Variable\", data=filling_features, palette=\"flare\")\n",
    "#Seuil pour suppression des varaibles\n",
    "plt.axvline(x=sup_threshold, linewidth=2, color = 'r')\n",
    "plt.text(sup_threshold+2,70, 'Seuil de suppression des variables', fontsize = 16, color = 'r')\n",
    "\n",
    "plt.title(\"Taux de remplissage des variables dans le jeu de données (%)\", fontdict=font_title)\n",
    "plt.xlabel(\"Taux de remplissage (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fd005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liste des variables à conserver\n",
    "features_to_conserve = list(filling_features.loc[filling_features['Taux_de_Null']>=sup_threshold, 'Variable'].values)\n",
    "#Liste des variables supprimées\n",
    "deleted_features = list(filling_features.loc[filling_features['Taux_de_Null']<sup_threshold, 'Variable'].values)\n",
    "\n",
    "#Nouveau Dataset avec les variables conservées\n",
    "Data = Data[features_to_conserve]\n",
    "Data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360b4c3",
   "metadata": {},
   "source": [
    "## 14.2 Remplacement des nan par la valeur médiane dans chaque colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505de311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Data.fillna(Data.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23438f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Features\n",
    "df.drop([\"SK_ID_CURR\" ], axis = 1).describe([.01, .1, .25, .5, .75, .8, .9, .95, .99])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse univariées--->Application Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d302100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Visualization for numerical variables\n",
    "#num_plot(df, num_cols=num_cols, remove=['SK_ID_CURR'], figsize = (15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyses Multivariées---> Application Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask,cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5)\n",
    "\n",
    "plt.title(f\"Heatmap des corrélations linéaires\\n\", fontsize = 18)\n",
    "plt.show()\n",
    "fig.savefig(\"Heatmap des corrélations linéaires_P4_2.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "corr_pairs = corr.unstack().sort_values(kind=\"quicksort\")\n",
    "strong_corr = (pd.DataFrame(corr_pairs[(abs(corr_pairs) > threshold)])\n",
    "               .reset_index().rename(columns={0:'corr_coeff'}))\n",
    "strong_corr = strong_corr[(strong_corr.index%2 == 0) & (strong_corr['level_0'] != strong_corr['level_1'])]\n",
    "strong_corr.sort_values('corr_coeff', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5eeb6d",
   "metadata": {},
   "source": [
    "## 14.3 Suppression des variables correlées de type level_0 sans la variable ['CNT_CHILDREN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop_Col=strong_corr.level_0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_children = [x for x in Drop_Col if x != \"CNT_CHILDREN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(no_children,axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27635017",
   "metadata": {},
   "source": [
    "## 14.4 Suppression de la variable  de type level_1[ 'CNT_FAM_MEMBERS'] correlée  à la variable de type level_0 ['CNT_CHILDREN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('CNT_FAM_MEMBERS',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd44027",
   "metadata": {},
   "source": [
    "## 14.5 Bilan et Evaluation de la taille de la dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70363a",
   "metadata": {},
   "source": [
    "# 15 Séparation des données en jeux de données  X_train et X_test aggrégées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91561536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset back into its training and testing segments\n",
    "X_train_agg = df[df['SK_ID_CURR'].isin(train_Sign)]\n",
    "X_test_agg = df[df['SK_ID_CURR'].isin(test_Sign)]\n",
    "X_test_agg.reset_index(drop=True, inplace=True)\n",
    "X_train_agg = pd.merge(X_train_agg,y_train_agg, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd34ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_agg.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dda84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_agg.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123a8d0",
   "metadata": {},
   "source": [
    "# 16 Preprocessing avant sauvegarde des données aggrégées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e65a5c",
   "metadata": {},
   "source": [
    "## 16.1Suppression la variable Credit_flag contenant des données inexploitables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_agg=X_train_agg.drop('Credit_flag',axis=1)\n",
    "X_test_agg=X_test_agg.drop('Credit_flag',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e61bcd",
   "metadata": {},
   "source": [
    "## 16.2 Insertion de la variable Age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_agg['Age']=(X_train_agg['DAYS_BIRTH']/365).round(1)\n",
    "X_test_agg['Age']=(X_test_agg['DAYS_BIRTH']/365).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a03e5e",
   "metadata": {},
   "source": [
    "## 16.3 Suppression  de la variable DAYS_BIRTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_agg=X_train_agg.drop('DAYS_BIRTH',axis=1)\n",
    "X_test_agg=X_test_agg.drop('DAYS_BIRTH',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17cbe5",
   "metadata": {},
   "source": [
    "## 16.4 Remplacement  de la colonne index par la colonne de type objet (SK_ID_CURR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=X_train_agg.set_index('SK_ID_CURR')\n",
    "df_test=X_test_agg.set_index('SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3310d",
   "metadata": {},
   "source": [
    "# 17 Conversion de toutes les colonnes numerique  en colonne positive dans les datasets aggrégées avant sauvegarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.columns[df.dtypes != np.object]] = df[df.columns[df.dtypes != np.object]].abs()\n",
    "#print(df)\n",
    "df_train[df_train.columns[df_train.dtypes !=np.object]]=df_train[df_train.columns[df_train.dtypes !=np.object]].abs()\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.columns[df_test.dtypes !=np.object]]=df_test[df_test.columns[df_test.dtypes !=np.object]].abs()\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dc3c3",
   "metadata": {},
   "source": [
    "# 18 Sauvegarde de train et test datasets aggrégées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01385509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('X_train_agg.csv',index=True)\n",
    "df_test.to_csv('X_test_agg.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bac677",
   "metadata": {},
   "source": [
    "# FIN D'ANALYSE EXPLORATION DES DONNEES EXTRAITES DES BASES DE DONNEES RELATIONNELLES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
